{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "PenguinFactory"
		},
		"Blob_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'Blob'"
		},
		"SQLdatabase_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'SQLdatabase'"
		},
		"UploadEvent_properties_typeProperties_scope": {
			"type": "string",
			"defaultValue": "/subscriptions/ff093b4d-e1b6-4506-b8f7-32420f93b811/resourceGroups/PenguinProject/providers/Microsoft.Storage/storageAccounts/inputblob"
		},
		"EventBased_properties_typeProperties_scope": {
			"type": "string",
			"defaultValue": "/subscriptions/ff093b4d-e1b6-4506-b8f7-32420f93b811/resourceGroups/PenguinProject/providers/Microsoft.Storage/storageAccounts/inputblob"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/Blob')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Blob storage of raw inputs, and the cleaned outputs prior to being sent to various sql tables.\nContains weight logs, Tag Logs, and individual weigh event logs.",
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('Blob_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SQLdatabase')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Database which contains tables for directQuery Connection to PowerBi workspace. Contains PenguinEventTable, TagID tables, and potentially WeighBridgeStepEvents",
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('SQLdatabase_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/UploadEvent')]",
			"type": "Microsoft.DataFactory/factories/triggers",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Calls the pipeline whenever a file is uploaded",
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [],
				"type": "BlobEventsTrigger",
				"typeProperties": {
					"blobPathBeginsWith": "/uploadblob/blobs/",
					"ignoreEmptyBlobs": true,
					"scope": "[parameters('UploadEvent_properties_typeProperties_scope')]",
					"events": [
						"Microsoft.Storage.BlobCreated"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/EventOutputCSV')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Stores transformed data for export into sql table PenguinEvent.",
				"linkedServiceName": {
					"referenceName": "Blob",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"container": "cleanedoutputs"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/Blob')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/PenguinEvent')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SQLdatabase",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "EventID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "Epoch",
						"type": "int",
						"precision": 10
					},
					{
						"name": "EventDate",
						"type": "date"
					},
					{
						"name": "EventTime",
						"type": "time",
						"scale": 7
					},
					{
						"name": "PenguinWeight",
						"type": "float",
						"precision": 15
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "PenguinEvent"
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/SQLdatabase')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/TagIDTable')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SQLdatabase",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "EventID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "Epoch",
						"type": "int",
						"precision": 10
					},
					{
						"name": "EventDate",
						"type": "date"
					},
					{
						"name": "EventTime",
						"type": "time",
						"scale": 7
					},
					{
						"name": "TagID",
						"type": "int",
						"precision": 10
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "TagID"
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/SQLdatabase')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/TagInput')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Blob",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "serial_log.csv",
						"container": "tagguano"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": [
					{
						"type": "String"
					},
					{
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/Blob')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/UploadedCSV')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Blob",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"container": "uploadblob"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/Blob')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/WeightLogCSV')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Blob",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "weight_log.csv",
						"container": "rawinputs"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": [
					{
						"type": "String"
					},
					{
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/Blob')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Upsert_CSV_to_SQL')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Iterates through CleanOutput in Blob container. Upserts the csv into the appropriate Table, then moves the csv into a historic folder.",
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EventOutputCSV",
								"type": "DatasetReference"
							},
							"name": "WeightLogCSV"
						},
						{
							"dataset": {
								"referenceName": "EventOutputCSV",
								"type": "DatasetReference"
							},
							"name": "TagIDCSV"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "PenguinEvent",
								"type": "DatasetReference"
							},
							"name": "SQLTable"
						},
						{
							"dataset": {
								"referenceName": "TagIDTable",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "AlterRow1"
						},
						{
							"name": "AlterRow2"
						}
					],
					"script": "source(output(\n\t\tEventID as short,\n\t\tEpoch as integer,\n\t\tEventDate as date,\n\t\tEventTime as timestamp 'HH:mm:ss',\n\t\tWeight as double\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: true,\n\tmoveFiles: ['','historic_cleaned_weights'],\n\twildcardPaths:['WeightLog*']) ~> WeightLogCSV\nsource(output(\n\t\tEventID as short,\n\t\tEpoch as integer,\n\t\tEventDate as date,\n\t\tEventTime as timestamp 'HH:mm:ss',\n\t\tTagID as integer\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: true,\n\tmoveFiles: ['','historic_clean_serial'],\n\twildcardPaths:['TagLog*'],\n\tpartitionBy('hash', 1)) ~> TagIDCSV\nWeightLogCSV alterRow(upsertIf(true())) ~> AlterRow1\nTagIDCSV alterRow(upsertIf(true())) ~> AlterRow2\nAlterRow1 sink(input(\n\t\tEventID as integer,\n\t\tEpoch as integer,\n\t\tEventDate as date,\n\t\tEventTime as timestamp,\n\t\tPenguinWeight as double\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:true,\n\tupsertable:true,\n\tkeys:['EventID'],\n\tformat: 'table',\n\tpreSQLs:['SET IDENTITY_INSERT [dbo].[PenguinEvent] ON'],\n\tmapColumn(\n\t\tEventID,\n\t\tEpoch,\n\t\tEventDate,\n\t\tEventTime,\n\t\tPenguinWeight = Weight\n\t)) ~> SQLTable\nAlterRow2 sink(input(\n\t\tEventID as integer,\n\t\tEpoch as integer,\n\t\tEventDate as date,\n\t\tEventTime as timestamp,\n\t\tTagID as integer\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:true,\n\tupsertable:true,\n\tkeys:['EventID'],\n\tformat: 'table',\n\tpreSQLs:['SET IDENTITY_INSERT [dbo].[TagID] ON'],\n\tmapColumn(\n\t\tEventID,\n\t\tEpoch,\n\t\tEventDate,\n\t\tEventTime,\n\t\tTagID\n\t)) ~> sink1"
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/EventOutputCSV')]",
				"[concat(variables('factoryId'), '/datasets/PenguinEvent')]",
				"[concat(variables('factoryId'), '/datasets/TagIDTable')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "UploadedCSV",
								"type": "DatasetReference"
							},
							"name": "WeightLogCSV"
						},
						{
							"dataset": {
								"referenceName": "UploadedCSV",
								"type": "DatasetReference"
							},
							"name": "RawTagDataCSV"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "EventOutputCSV",
								"type": "DatasetReference"
							},
							"name": "Sink"
						},
						{
							"dataset": {
								"referenceName": "EventOutputCSV",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "DateTimeColumn"
						},
						{
							"name": "GenerateSurrogateKey"
						},
						{
							"name": "Filter1"
						},
						{
							"name": "Select1"
						},
						{
							"name": "Date"
						},
						{
							"name": "CleaningandDateTime"
						},
						{
							"name": "CleanedData"
						},
						{
							"name": "ColumnSorting"
						},
						{
							"name": "ReOrderColumns"
						},
						{
							"name": "SurrogateKey1"
						},
						{
							"name": "Select2"
						},
						{
							"name": "TagIDextraction"
						}
					],
					"script": "source(output(\n\t\tEpoch as integer,\n\t\tWeight as double,\n\t\tJunk as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: true,\n\tmoveFiles: ['','historic_weight'],\n\twildcardPaths:['weight*.csv'],\n\tpartitionBy('hash', 1)) ~> WeightLogCSV\nsource(output(\n\t\tEpoch as integer,\n\t\tMessage as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: true,\n\tmoveFiles: ['','historic_serial'],\n\twildcardPaths:['serial*.csv']) ~> RawTagDataCSV\nWeightLogCSV derive(Date = trim(left(toString(toTimestamp(seconds(Epoch))),10)),\n\t\tTime = trim(right(toString(toTimestamp(seconds(Epoch))),8))) ~> DateTimeColumn\nColumnSorting keyGenerate(output(EventID as long),\n\tstartAt: 1L,\n\tpartitionBy('hash', 1)) ~> GenerateSurrogateKey\nGenerateSurrogateKey filter(0.8<Weight\r\n&&Weight<10) ~> Filter1\nFilter1 select(mapColumn(\n\t\tEpoch,\n\t\tWeight,\n\t\tDate,\n\t\tTime,\n\t\tEventID\n\t),\n\tpartitionBy('hash', 1),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> Select1\nRawTagDataCSV filter(startsWith(Message, 'TAG')) ~> Date\nTagIDextraction derive(Date = trim(left(toString(toTimestamp(seconds(Epoch))),10)),\n\t\tTime = trim(right(toString(toTimestamp(seconds(Epoch))),8))) ~> CleaningandDateTime\nCleaningandDateTime alterRow(upsertIf(true())) ~> CleanedData\nDateTimeColumn select(mapColumn(\n\t\tEpoch,\n\t\tWeight,\n\t\tDate,\n\t\tTime\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> ColumnSorting\nSelect1 select(mapColumn(\n\t\tEventID,\n\t\tEpoch,\n\t\tDate,\n\t\tTime,\n\t\tWeight\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> ReOrderColumns\nCleanedData keyGenerate(output(EventID as long),\n\tstartAt: 1L) ~> SurrogateKey1\nSurrogateKey1 select(mapColumn(\n\t\tEventID,\n\t\tEpoch,\n\t\tDate,\n\t\tTime,\n\t\tMessage\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> Select2\nDate derive(Message = toInteger(trim(right(trim(left(Message,7)),2)))) ~> TagIDextraction\nReOrderColumns sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:[(concat(toString(\"WeightLog_\") ,toString(currentDate())))],\n\tmapColumn(\n\t\tEventID,\n\t\tEpoch,\n\t\tDate,\n\t\tTime,\n\t\tWeight\n\t),\n\tpartitionBy('hash', 1)) ~> Sink\nSelect2 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:[(concat(toString(\"TagLog_\") ,toString(currentDate())))],\n\tmapColumn(\n\t\tEventID,\n\t\tEpoch,\n\t\tDate,\n\t\tTime,\n\t\tMessage\n\t),\n\tpartitionBy('hash', 1)) ~> sink1"
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/UploadedCSV')]",
				"[concat(variables('factoryId'), '/datasets/EventOutputCSV')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/pipeline1')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "CleaningofData",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "dataflow1",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"WeightLogCSV": {},
									"RawTagDataCSV": {},
									"Sink": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							}
						}
					},
					{
						"name": "Upsert_CSV_to_SQL",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Upsert_CSV_to_SQL",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"WeightLogCSV": {},
									"TagIDCSV": {},
									"SQLTable": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							}
						}
					}
				],
				"annotations": [],
				"lastPublishTime": "2020-10-08T11:53:25Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/dataflow1')]",
				"[concat(variables('factoryId'), '/dataflows/Upsert_CSV_to_SQL')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Schedule')]",
			"type": "Microsoft.DataFactory/factories/triggers",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Triggers pipeline, once a day at 12pm.",
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "pipeline1",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2020-10-12T08:24:00.000Z",
						"timeZone": "UTC",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								10
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/pipelines/pipeline1')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/EventBased')]",
			"type": "Microsoft.DataFactory/factories/triggers",
			"apiVersion": "2018-06-01",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "pipeline1",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "BlobEventsTrigger",
				"typeProperties": {
					"blobPathBeginsWith": "/uploadblob/blobs/up",
					"blobPathEndsWith": ".csv",
					"ignoreEmptyBlobs": true,
					"scope": "[parameters('EventBased_properties_typeProperties_scope')]",
					"events": [
						"Microsoft.Storage.BlobCreated"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/pipelines/pipeline1')]"
			]
		}
	]
}